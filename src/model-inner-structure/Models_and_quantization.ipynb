{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNt+9tyHeZZKx8HhcFIRSsy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vilasha/ollama-sandbox/blob/master/src/model-inner-structure/Models_and_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization of open source model, inference with HuggingFace lower-level API\n",
        "\n",
        "Models in transformers API are lower-level of running inference with LLMs (opposite to pipelines which are higher-level of abstraction). Models wrap PyTorch code for the transformers themselves.\n",
        "\n",
        "Here we will download an open source model to the local hard drive of the runtime (GPU runtime is required for this Notebook), quantify it twice from 32 bits to 4 bits (decrease presicion of weights), and then run inference with this scaled down model.\n",
        "\n",
        "If Llama models give 401-unauthorized or 403-forbidden response, please go to the HuggingFace to [any model of Llama-3.2 family](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) and sign terms and conditions. Once access is granted, in your [Gated Repositories](https://huggingface.co/settings/gated-repos) page this model will appear with `Request-status=Accepted`"
      ],
      "metadata": {
        "id": "zMs2H5KNoXAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some preparation\n",
        "!pip install -q --upgrade bitsandbytes accelerate\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# login to HuggingFace\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "# define model names\n",
        "LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "PHI = \"microsoft/Phi-4-mini-instruct\"\n",
        "GEMMA = \"google/gemma-2-2b-it\"\n",
        "QWEN = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "DEEPSEEK = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "messages = [ {\"role\": \"user\", \"content\": \"Tell a joke about AI hype\"} ]\n",
        "\n",
        "# Quantization Config - this allows us to load the model into memory and use less memory\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# define function that tokenizes messages into embedded vector, downloads a model\n",
        "# from a pretrained HuggingFace model, scales it down according to quant_config\n",
        "# above (4 bits per weight)\n",
        "def generate(model_name, messages, quant=True, max_new_tokens=80):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  if quant:\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config).to(\"cuda\")\n",
        "  else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "  memory = model.get_memory_footprint() / 1e6\n",
        "  print(f\"\\nMemory footprint: {memory:,.1f} MB\\n\")\n",
        "\n",
        "  print(\"\\nInternal representation of the model:\\n\")\n",
        "  print(model)\n",
        "\n",
        "  print(\"\\nResponse including system tokens:\\n\")\n",
        "  # we use streamer here, so it automatically prints generated text in streaming mode\n",
        "  outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, streamer=streamer)\n",
        "  del model\n",
        "  del tokenizer\n",
        "  del input_ids\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "PcXC-6nmrKgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Facebook Llama\n",
        "generate(LLAMA, messages)"
      ],
      "metadata": {
        "id": "Ju_EAX4hu_Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Microsoft Phi\n",
        "generate(PHI, messages)"
      ],
      "metadata": {
        "id": "IQlWRYr6va9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Gemma\n",
        "generate(GEMMA, messages)"
      ],
      "metadata": {
        "id": "3z91f4xCvcZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alibaba Qwen\n",
        "generate(QWEN, messages)"
      ],
      "metadata": {
        "id": "UWRW6chCvc_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSeek\n",
        "generate(DEEPSEEK, messages)"
      ],
      "metadata": {
        "id": "hKX-MMtNv1-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}