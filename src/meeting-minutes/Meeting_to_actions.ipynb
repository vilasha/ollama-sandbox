{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyOXCQahbe8VLR2MuzONRhcM",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2ff3caa3c07f47f696757d39863a0130": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8849f3c121534b59813a11597dd81ce1",
       "IPY_MODEL_0f9cd7245f4e4e21b39a7b4d31632f0f",
       "IPY_MODEL_58653c5800ef46e28525627e09b197da"
      ],
      "layout": "IPY_MODEL_81fd11c6eab64db4837e1d3916c81884"
     }
    },
    "8849f3c121534b59813a11597dd81ce1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_581c530d525f4f0c83a95bc6aa69c2a5",
      "placeholder": "​",
      "style": "IPY_MODEL_39ffb7e877464afa89a7b95da0697c60",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "0f9cd7245f4e4e21b39a7b4d31632f0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36d90b129a8840aaa21dbf459761507a",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fed9940d12b042fe9ab30fba6482a12e",
      "value": 2
     }
    },
    "58653c5800ef46e28525627e09b197da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9347622c2b6046daa24c50cc2dafb412",
      "placeholder": "​",
      "style": "IPY_MODEL_8e2014803e7f41e292f4e5d770678afd",
      "value": " 2/2 [00:29&lt;00:00, 13.38s/it]"
     }
    },
    "81fd11c6eab64db4837e1d3916c81884": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "581c530d525f4f0c83a95bc6aa69c2a5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39ffb7e877464afa89a7b95da0697c60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36d90b129a8840aaa21dbf459761507a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fed9940d12b042fe9ab30fba6482a12e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9347622c2b6046daa24c50cc2dafb412": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e2014803e7f41e292f4e5d770678afd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vilasha/ollama-sandbox/blob/master/Meeting_to_actions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert meeting recording (audio only) to minutes and actions\n",
    "\n",
    "This Colab notebook implements an end-to-end pipeline for transcribing a meeting audio file and generating structured meeting minutes with action items using open-source models.\n",
    "\n",
    "Key components:\n",
    "\n",
    "1. Speech-to-Text Transcription\n",
    "   Loads an English audio file (MP3) from Google Drive.\n",
    "   Uses OpenAI's whisper-small.en via Hugging Face's pipeline(\"automatic-speech-recognition\") in FP16 on GPU to produce a full transcript (timestamps available but unused).\n",
    "\n",
    "2. Meeting Minutes Generation\n",
    "   Prompts Meta's Llama-3.2-3B-Instruct (loaded with 4-bit quantization via BitsAndBytes for efficient GPU inference) to convert the raw transcript into structured markdown minutes.\n",
    "   The prompt requests summary (including attendees, location, date), discussion points, takeaways, and action items with owners.\n",
    "   Applies a chat template, uses a TextStreamer for live output, and renders the final response as Markdown.\n",
    "\n",
    "Supporting setup:\n",
    "   Mounts Google Drive, logs into Hugging Face (via stored token), installs/upgrades bitsandbytes and accelerate for quantization support.\n",
    "\n",
    "The workflow is lightweight, runs on free Colab resources (enabled by 4-bit quantization of the 3B model), and combines a strong ASR model with a compact instruction-tuned LLM for summarization and action-item extraction."
   ],
   "metadata": {
    "id": "qRsGAcUDEQzi"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2ff3caa3c07f47f696757d39863a0130",
      "8849f3c121534b59813a11597dd81ce1",
      "0f9cd7245f4e4e21b39a7b4d31632f0f",
      "58653c5800ef46e28525627e09b197da",
      "81fd11c6eab64db4837e1d3916c81884",
      "581c530d525f4f0c83a95bc6aa69c2a5",
      "39ffb7e877464afa89a7b95da0697c60",
      "36d90b129a8840aaa21dbf459761507a",
      "fed9940d12b042fe9ab30fba6482a12e",
      "9347622c2b6046daa24c50cc2dafb412",
      "8e2014803e7f41e292f4e5d770678afd"
     ]
    },
    "id": "Ha9x6v675Bxi",
    "outputId": "d04041cd-859b-4680-dbf4-52668f71a479"
   },
   "source": [
    "!pip install -q --upgrade bitsandbytes accelerate\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Define models to call. For speech recognition we will be using WhisperSmall\n",
    "# from OpenAI and for converting meeting minutes to action items Llama from Meta\n",
    "WHISPER = \"openai/whisper-small.en\"\n",
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Connect Google Drive, it becomes accessible like if it was mounted local disk\n",
    "# On the first run of the Notebook there will be a pop-up message asking to\n",
    "# allow to connect to the Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "# I saved the file at https://drive.google.com/file/d/1P86u__2pIJ9E8GAdw4RHFU0Ry_Y4LPZE/view?usp=sharing\n",
    "# In my Google Drive it resides at /Colab Notebooks/src/council_010421_2022101V.mp3\n",
    "# The file is from one of the HuggingFace datasets: https://huggingface.co/datasets/huuuyeah/MeetingBank_Audio/tree/main\n",
    "audio_filename = \"/content/drive/MyDrive/Colab Notebooks/src/council_010421_2022101V.mp3\"\n",
    "\n",
    "# Sign in to HuggingFace Hub\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)\n",
    "\n",
    "# Open the file\n",
    "audio_file = open(audio_filename, \"rb\")\n",
    "\n",
    "# Speech recognition\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=WHISPER,\n",
    "    dtype=torch.float16,\n",
    "    device='cuda',\n",
    "    return_timestamps=True\n",
    ")\n",
    "\n",
    "result = pipe(audio_filename)\n",
    "transcription = result[\"text\"]\n",
    "print(transcription)\n",
    "\n",
    "# Convert transcription to action items\n",
    "system_message = \"\"\"\n",
    "You produce minutes of meetings from transcripts, with summary, key discussion points,\n",
    "takeaways and action items with owners, in markdown format without code blocks.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Below is an extract transcript of a meeting.\n",
    "Please write minutes in markdown without code blocks, including:\n",
    "- a summary with attendees, location and date\n",
    "- discussion points\n",
    "- takeaways\n",
    "- action items with owners\n",
    "\n",
    "Transcription:\n",
    "{transcription}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]\n",
    "\n",
    "# Quantify Llama model, so it works faster and takes less space on the local disk\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
    "outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "display(Markdown(response))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
